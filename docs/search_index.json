[["data-science-in-julia-for-hackers.html", "Data Science in Julia for Hackers Chapter 1 Data Science in Julia for Hackers 1.1 # Prologue", " Data Science in Julia for Hackers LambdaE 2021-04-14 Chapter 1 Data Science in Julia for Hackers This book is currently in a beta version. We are looking forward to getting feedback and criticism, which you can mail to martina.cantaro@lambdaclass.com Thank you! Bayes hacking This book is written by Federico Carrone, Herman Obst Demaestri and Mariano Nicolini. Thanks to Martina Cantaro, Camilo Plata, Manuel Puebla, Lucas Raúl Fernandez Piana, Osvaldo Martin, Iñaki Garay and Mariana Vinyolas. 1.1 # Prologue We have a habit in writing articles published in scientific journals to make the work as finished as possible, to cover up all the tracks, to not worry about the blind alleys or describe how you had the wrong idea first, and so on - Richard Feynman Only that what’s open can be true. Full transparency is best for me and you - OpenBSD One of the first things to note about this book is that it is not an academic textbook. The authors of this book are not academics but a multidisciplinary team of passionate, amateur practitioners from different backgrounds, namely Engineering, Computer Science, Physics and Economy, that found a common ground to write this book, and that day by day keep reading, learning and applying new approaches, technologies and ways of thinking. This book lies somewhere in between a methodological recipe and a theoretical intensive textbook. What we want to deliver is a mathematical and computational methodology to face concrete Data Science problems, that is, applying theory and science to real-world problems involving data. The relationship between theory and practice is complex. Considering them as a whole can take us much farther. These pages may offer the theorist a way to think about problematic situations in a more down to earth manner, and to the practitioner, stimulation to go beyond the mere application of programming libraries and tools. As the name of the book states, this is a book for Hackers. The term can have opposite connotations, depending on who is pronouncing it and to whom it refers to. In media and pop culture, it is associated with cyber-criminals, people that use computers and technology with malicious intent. In the cyber-security domain, hackers are, as stated in the final chapter of Hacking: The Art of Exploitation by Jon Erickson, “…just people with innovative spirits and an in-depth knowledge of technology”. But the definition we like the most is borrowed from The Jargon File’s glossary, written by Eric S. Raymond, &gt; “A person who delights in having an intimate understanding of the internal workings of a system, computers and computer networks in particular.” It is in this sense that this book is meant for hackers: it will lead you down a road with a results-driven perspective, slowly growing intuition about the inner workings of many problems involving data and what they all have in common, with an emphasis on application. The name of the book is also inspired by the great Bayesian Methods for Hackers, which had a big influence on the topics and the approach of this book. The situations presented in the book are diverse, reflecting the broad spectrum of application of Data Science. The solutions, however, don’t try to be beautiful or perfect. We know the path to resolving real-life problems is muddy, that sometimes you might feel lost and that your ideas may change as you try to find the solution, but we encourage to embrace these coarse features and accept the sharp corners in this learning-through-solving journey. Suggestions, criticism and feedback are very appreciated, this book is alive and will evolve as a consequence of its environment’s response. Although this book isn’t intended to have obscure, overly technical or academic definitions that may put the reader off, people with a little programming knowledge, a basic understanding of Calculus and some modelling intuition will get the most out of these pages. Familiarity with a high-level language like Python, taking derivatives and some function analysis should be enough for the reader to follow through with the book. All chapters include the code for the reader to play, explore and implement the solutions on their own. Moreover, links to notebooks with the worked solutions are available. Why Data Science in Julia? There are several reasons why Julia is great language for Data Science. Julia code is readable, specially in regards to math-related computations. Julia is designed for math from the ground up, and considering the great amounts of math involved in Data Science, this makes it a very convenient programming language to work with. Another interesting aspect of Julia is its syncretism. This is due to its historical background: it was developed by a team of scientists who wanted a tool to address problems they typically encountered while doing research. Often, scientists are faced with complicated situations that need to integrate various tools and find an equilibrium point among many requirements, like learning to program in a new language and to efficiently use the equipment and hardware available. Julia is the perfect tool for these needs, suited for implementing high-level solutions than can be tested easily and quickly in an interactive manner, and allowing to write highly performant code in a simple way when needed, all in the same language. Moreover, features like multiple-dispatch and simple syntax, allow for great composability between packages as well as scalability, making the task of writing software much more user friendly and maintainable. "],["basketball-how-likely-is-it-to-score.html", "Chapter 2 Basketball: how likely is it to score? 2.1 Prior Predictive Checks: Part I 2.2 New model and prior predictive checks: Part II", " Chapter 2 Basketball: how likely is it to score? begin using Random Random.seed!(1234) end; begin using CSV using DataFrames using StatsPlots using Plots season_2007 = CSV.read(&quot;./data/seasons/shots_season_2006_2007.csv&quot;, DataFrame) #shots = vcat(season_2007, season_2008, season_2009); shots = season_2007 shots[!, :distance] = sqrt.( shots.x .^ 2 + shots.y .^ 2) shots[!, :angle] = atan.( shots.y ./ shots.x ) filter!(x-&gt; x.distance&gt;1, shots) end; begin using Images img = load(&quot;./images/basket_court.png&quot;) plot(img) end begin using Turing using StatsFuns end When playing basketball we can ask ourself: how likely is it to score given a position in the court? To answer this question we are going to use data from NBA games from the season 2006 - 2007. We will consider all types of shots. first(shots) ## DataFrameRow ## Row │ result x y period time distance angle ## │ Int64 Int64 Float64 Int64 Time… Float64 Float64 ## ─────┼─────────────────────────────────────────────────────────────── ## 1 │ 0 -14 16.75 1 11:42:00 21.8303 -0.874592 But how do we interpret the data? Below we show a sketch of a basketball court, its dimensions and how to interpret the data in the table. So, the x and y axis have their origin at the hoop, and we compute the distance from this point to where the shot was made. Also, we compute the angle with respect to the x axis, showed as θ in the sketch. In the data we have the period, which can take values from 1 to 4, meaning the period in which the shot was made. We now plot where the shots where made: begin histogram2d(shots.y[1:10000], shots.x[1:10000], bins=(50,30)) ylabel!(&quot;y axis&quot;) xlabel!(&quot;x axis&quot;) end We see that the shots are very uniformly distributed near the hoop, except for distances very near to the hoop, to see this better, we plot the histograms for each axis, x and y. But we are interested in the shots that were scored, so we filter now the shots made and plot the histogram of each axis. shots_made = filter(x-&gt;x.result==1, shots); begin using StatsBase h = fit(Histogram, (shots_made.y, shots_made.x), nbins=40) #plot(h) # same as histogram2d wireframe(midpoints(h.edges[2]), midpoints(h.edges[1]), h.weights, zlabel=&quot;counts&quot;, xlabel=&quot;y&quot;, ylabel=&quot;x&quot;, camera=(40,40)) title!(&quot;Histogram of shots scored&quot;) end begin histogram(shots_made.y[1:10000], legend=false, nbins=40) xlabel!(&quot;x axis&quot;) ylabel!(&quot;Counts&quot;) end begin histogram(shots_made.x[1:10000], legend=false, nbins=45) xlabel!(&quot;y axis&quot;) ylabel!(&quot;Counts&quot;) end If we plot a 3d plot of the count we obtain the plot wireplot shown below. We see that more shot are made as we get near the hoop, as expected. It is important to notice that we are not showing the probability of scoring, we are just showing the distribution of shot scored, not how likely is it to score. 2.0.1 Modeling the probability of scoring The first model we are going to propose is a Bernoulli model. Why a Bernoulli Distribution? A Bernoulli Distribution results from an experiment in which we have 2 possible outcomes, one that we usually called a success and another called a fail. In our case our success is scoring the shot and the other possible event is failing it. The only parameter needed in a bernoulli distribution is the probability p of having a success. We are going to model this parameter as a logistic function: begin plot(logistic, legend=false) ylabel!(&quot;Probability&quot;) xlabel!(&quot;x&quot;) title!(&quot;Logistic function (x)&quot;) end Why a logistic function? We are going to model the probability of shoot as a function of some variables, for example the distance to the hoop, and we want that our probability of scoring increases as we are getting closer to it. Also out probability needs to be between 0 an 1, so a nice function to map our values is the logistic function. So, the model we are going to propose is: \\(p\\sim logistic(a+ b*distance[i] + c*angle[i])\\) \\(outcome[i]\\sim Bernoulli(p)\\) But what values and prior distributions are we going to propose to the parameters a, b and c? Let’s see: 2.1 Prior Predictive Checks: Part I Suppose we say that our prior distributions for a, b and c are going to be 3 gaussian distributions with mean 0 and variance 1. Lets sample and see what are the possible posterior distributions for our probability of scoring p: \\(a\\sim N(0,1)\\) \\(b\\sim N(0,1)\\) \\(c\\sim N(0,1)\\) begin possible_distances = 0:0.01:1 possible_angles = 0:0.01:π/2 angle = π/2 n_samples = 100 a_prior_sampling = rand(Normal(0,1), n_samples) b_prior_sampling = rand(Normal(0,1), n_samples) predicted_p = [] for i in 1:n_samples push!(predicted_p, logistic.(a_prior_sampling[i] .+ b_prior_sampling[i].*possible_distances)) end end begin plot(possible_distances, predicted_p[1], legend = false, color=&quot;blue&quot;) for i in 2:n_samples plot!(possible_distances, predicted_p[i], color=:blue) end xlabel!(&quot;Normalized distance&quot;) ylabel!(&quot;Predicted probability&quot;) title!(&quot;Prior predictive values for p&quot;) current() end We see that some of the predicted behaviours for p don’t make sense. For example, if b takes positive values, we are saying that as we increase our distance from the hoop, the probability of scoring also increase. So we propose instead the parameter b to be the negative values of a LogNormal distribution. The predicted values for p are shown below. So our model now have as priors distributions: \\(a\\sim Normal(0,1)\\) \\(b\\sim LogNormal(1,0.25)\\) \\(c\\sim Normal(0,1)\\) and sampling values from those prior distributions, we obtain the plot shown below for the predicted values of p. begin b_prior_sampling_negative = rand(LogNormal(1,0.25), n_samples) predicted_p_inproved = [] for i in 1:n_samples push!(predicted_p_inproved, logistic.(a_prior_sampling[i] .- b_prior_sampling_negative[i].*possible_distances)) end end begin plot(possible_distances, predicted_p_inproved[1], legend = false, color=:blue) for i in 2:n_samples plot!(possible_distances, predicted_p_inproved[i], color=:blue) end xlabel!(&quot;Normalized distance&quot;) ylabel!(&quot;Predicted probability&quot;) title!(&quot;Prior predictive values for p with negative LogNormal prior&quot;) current() end Now that we have the expected behaviour for p, we define our model and calculate the posterior distributions with our data points. 2.1.1 Defining our model and computing posteriors Now we define our model to sample from it: @model logistic_regression(distances, angles, result,n) = begin N = length(distances) # Set priors. a ~ Normal(0,1) b ~ LogNormal(1,0.25) c ~ Normal(0,1) for i in 1:n p = logistic( a - b*distances[i] + c*angles[i]) result[i] ~ Bernoulli(p) end end ## logistic_regression (generic function with 1 method) n=1000; The output of the sampling tell us also some information about sampled values for our parameters, like the mean, the standard deviation and some other computations. # Sample using HMC. chain = mapreduce(c -&gt; sample(logistic_regression(shots.distance[1:n] ./ maximum(shots.distance[1:n] ), shots.angle[1:n], shots.result[1:n], n), NUTS(), 1500), chainscat, 1:3 ); 2.1.1.1 Traceplot In the plot below we show a traceplot of the sampling. What is a traceplot? When we run a model and calculate the posterior, we obtain sampled values from the posterior distributions. We can tell our sampler how many sampled values we want. A traceplot is just showing them in sequential order. We also can plot the distribution of those values, and this is what is showed next to each traceplot. plot(chain, dpi=60) begin a_mean = mean(chain[:a]) b_mean = mean(chain[:b]) c_mean = mean(chain[:c]) end; Now plotting the probability of scoring using the posterior distributions of a, b and c for an angle of 45°, we obtain: begin p_constant_angle = [] for i in 1:length(chain[:a]) push!(p_constant_angle, logistic.(chain[:a][i] .- chain[:b][i].*possible_distances .+ chain[:c][i].*π/4)); end end begin plot(possible_distances,p_constant_angle[1], legend=false, alpha=0.1, color=:blue) for i in 2:1000 plot!(possible_distances,p_constant_angle[i], alpha=0.1, color=:blue) end xlabel!(&quot;Normalized distance&quot;) ylabel!(&quot;Probability&quot;) title!(&quot;Scoring probability vs Normalized distance (angle=45°)&quot;) current() end The plot shows that the probability of scoring is higher as our distance to the hoop decrease, which makes sense, since the difficulty of scoring increase. We plot now how the probability varies with the angle for a given distance. Here we plot for a mid distance, corresponding to 0.5 in a normalized distance. begin p_constant_distance = [] for i in 1:length(chain[:a]) push!(p_constant_distance, logistic.(chain[:a][i] .- chain[:b][i].*0.5 .+ chain[:c][i].*possible_angles)); end end begin plot(rad2deg.(possible_angles),p_constant_distance[1], legend=false, alpha=0.1, color=:blue) for i in 2:1000 plot!(rad2deg.(possible_angles),p_constant_distance[i], alpha=0.1, color=:blue) end xlabel!(&quot;Angle [deg]&quot;) ylabel!(&quot;Probability&quot;) title!(&quot;Scoring probability vs Angle (mid distance)&quot;) current() end We see that the model predict an almost constant probability for the angle. 2.2 New model and prior predictive checks: Part II Now we propose another model with the form: \\(p\\sim logistic(a+ b^{distance[i]} + c*angle[i])\\) *But for what values of b the model makes sense? We show below the plot for 4 function with 4 possible values of b, having in mind that the values of x, the normalized distance, goes from 0 to 1. begin f1(x) = 0.3^x f2(x) = 1.5^x f3(x) = -0.3^x f4(x) = -1.5^x end; begin plot(0:0.01:1, f1, label=&quot;f1: b&lt;1 &amp; b&gt;0&quot;, xlim=(0,1), ylim=(-2,2), lw=3) plot!(0:0.01:1, f2, label=&quot;f2: b&gt;1&quot;, lw=3) plot!(0:0.01:1, f3, label=&quot;f3: b&lt;0 &amp; b&gt;-1&quot;, lw=3) plot!(0:0.01:1, f4, label=&quot;f3: b&lt;-1&quot;, lw=3) xlabel!(&quot;Normalized distance&quot;) title!(&quot;Prior Predictive influence of distance&quot;) end Analysing the possible values for b, the one that makes sense is the value proposed in f1, since we want an increasing influence of the distance in the values of p as the distance decreases, since the logistic function has higher values for higher values of x. So now that we know the values the our parameter b can take, we propose for it a beta distribution with parameters α=2 and β=5, showed in the plot below. begin plot(Beta(2,5), xlim=(-0.1,1), legend=false) title!(&quot;Prior distribution for b&quot;) end 2.2.1 Defining the new model and computing posteriors We define then our model and calculate the posterior as before. @model logistic_regression_exp(distances, angles, result, n) = begin N = length(distances) # Set priors. a ~ Normal(0,1) b ~ Beta(2,5) c ~ Normal(0,1) for i in 1:n p = logistic( a + b .^ distances[i] + c*angles[i]) result[i] ~ Bernoulli(p) end end ## logistic_regression_exp (generic function with 1 method) # Sample using HMC. chain_exp = mapreduce(c -&gt; sample(logistic_regression_exp(shots.distance[1:n] ./ maximum(shots.distance[1:n] ), shots.angle[1:n], shots.result[1:n], n), HMC(0.05, 10), 1500), chainscat, 1:3 ); Plotting the traceplot we see again that the variable angle has little importance since the parameter c, that can be related to the importance of the angle variable for the probability of scoring, is centered at 0. plot(chain_exp, dpi=55) begin p_exp_constant_angle = [] for i in 1:length(chain_exp[:a]) push!(p_exp_constant_angle, logistic.(chain_exp[:a][i] .+ chain_exp[:b][i].^possible_distances .+ chain_exp[:c][i].*π/4)); end end Employing the posteriors distributions computed, we plot the probability of scoring as function of the normalized distance and obtain the plot shown below. begin plot(possible_distances,p_exp_constant_angle[1], legend=false, alpha=0.1, color=:blue) for i in 2:1000 plot!(possible_distances,p_exp_constant_angle[i], alpha=0.1, color=:blue) end xlabel!(&quot;Normalized distance&quot;) ylabel!(&quot;Probability&quot;) title!(&quot;Scoring probability vs Normalized distance (angle=45°)&quot;) current() end Given that we have 2 variables, we can plot the mean probability of scoring as function of the two and obtain a surface plot. We show this below. begin angle_ = collect(range(0, stop=π/2, length=100)) dist_ = collect(range(0, stop=1, length=100)) it = Iterators.product(angle_, dist_) matrix = collect.(it) values = reshape(matrix, (10000, 1)) angle_grid = getindex.(values,[1]); dist_grid = getindex.(values,[2]); z = logistic.(mean(chain_exp[:a]) .+ mean(chain_exp[:b]).^dist_grid .+ mean(chain_exp[:c]).*angle_grid); end; im3 = load(&quot;./images/img1.png&quot;) ## 771×1175 Array{RGBA{N0f8},2} with eltype RGBA{Normed{UInt8,8}}: ## RGBA{N0f8}(1.0,1.0,1.0,1.0) … RGBA{N0f8}(1.0,1.0,1.0,1.0) ## RGBA{N0f8}(1.0,1.0,1.0,1.0) RGBA{N0f8}(1.0,1.0,1.0,1.0) ## RGBA{N0f8}(1.0,1.0,1.0,1.0) RGBA{N0f8}(1.0,1.0,1.0,1.0) ## RGBA{N0f8}(1.0,1.0,1.0,1.0) RGBA{N0f8}(1.0,1.0,1.0,1.0) ## RGBA{N0f8}(1.0,1.0,1.0,1.0) RGBA{N0f8}(1.0,1.0,1.0,1.0) ## RGBA{N0f8}(1.0,1.0,1.0,1.0) … RGBA{N0f8}(1.0,1.0,1.0,1.0) ## RGBA{N0f8}(1.0,1.0,1.0,1.0) RGBA{N0f8}(1.0,1.0,1.0,1.0) ## RGBA{N0f8}(1.0,1.0,1.0,1.0) RGBA{N0f8}(1.0,1.0,1.0,1.0) ## RGBA{N0f8}(1.0,1.0,1.0,1.0) RGBA{N0f8}(1.0,1.0,1.0,1.0) ## RGBA{N0f8}(1.0,1.0,1.0,1.0) RGBA{N0f8}(1.0,1.0,1.0,1.0) ## ⋮ ⋱ ## RGBA{N0f8}(1.0,1.0,1.0,1.0) RGBA{N0f8}(1.0,1.0,1.0,1.0) ## RGBA{N0f8}(1.0,1.0,1.0,1.0) RGBA{N0f8}(1.0,1.0,1.0,1.0) ## RGBA{N0f8}(1.0,1.0,1.0,1.0) RGBA{N0f8}(1.0,1.0,1.0,1.0) ## RGBA{N0f8}(1.0,1.0,1.0,1.0) … RGBA{N0f8}(1.0,1.0,1.0,1.0) ## RGBA{N0f8}(1.0,1.0,1.0,1.0) RGBA{N0f8}(1.0,1.0,1.0,1.0) ## RGBA{N0f8}(1.0,1.0,1.0,1.0) RGBA{N0f8}(1.0,1.0,1.0,1.0) ## RGBA{N0f8}(1.0,1.0,1.0,1.0) RGBA{N0f8}(1.0,1.0,1.0,1.0) ## RGBA{N0f8}(1.0,1.0,1.0,1.0) RGBA{N0f8}(1.0,1.0,1.0,1.0) ## RGBA{N0f8}(1.0,1.0,1.0,1.0) … RGBA{N0f8}(1.0,1.0,1.0,1.0) plot(im3) #begin #plotly() #plot(dist_grid, rad2deg.(angle_grid), z, color=:blue, zlabel=&quot;Probability&quot;, legend=false, camera=(10,0)) #xlabel!(&quot;Normalized distance&quot;) #ylabel!(&quot;Angle [deg]&quot;) #title!(&quot;Mean Probability of scoring from posterior sampled values&quot;) #end The plot show the behaviour expected, an increasing probability of scoring as we get near the hoop. We also see that there is almost no variation of the probability with the angle. 2.2.2 Summary In this chapter, we used the NBA shooting data of the season 2006-2007 to analyze how the scoring probability is affected by some variables, such as the distance from the hoop and the angle of shooting. First, we inspected the data by plotting a heatplot of all the shots made and making histograms of the ones that scored. As our goal was to study the probability of scoring, which is a Bernoulli trial situation, we decided to use a Bernoulli model. Since the only parameter needed in a Bernoulli distribution is the probability p of having a success, we modeled p as a logistic function: \\(p\\sim logistic(a+ b*distance[i] + c*angle[i])\\) We set the prior probability of the parameters a and c to a normal distribution and b to a log-normal one. Thus, we constructed our logistic regression model and sampled it using the Markov Monte Carlo algorithm. To gain a better understanding of the sampling process, we made a traceplot that shows the sampled values in a sequential order. Later, we decided to try with a more complex logistic regression model, similar to the first one but this time modifying the distance parameter: \\(p\\sim logistic(a+ b^{distance[i]} + c*angle[i])\\) We set the prior distribution of b to a beta distribution and constructed the second logistic regression model, sampled it and plotted the results. Finally, we analyzed the results to see if the period of the game affects the probability of scoring. ### Give us feedback This book is currently in a beta version. We are looking forward to getting feedback and criticism: * Submit a GitHub issue here. * Mail us to martina.cantaro@lambdaclass.com Thank you! date: “4/14/2021” output: html_document — "],["premier-league-analysis.html", "Chapter 3 Premier League analysis 3.1 Creating our conjectures 3.2 Summary", " Chapter 3 Premier League analysis begin using JSON using DataFrames end using Turing using LinearAlgebra 3.1 Creating our conjectures If there is one thing we can all agree on, it is that the reality in which we live is complex. The explanation for the things we usually see, and have naturalized in our daily lives, is usually quite complex and requires abstraction from what “is simply seen”. In order to give an explanation and gain a deeper understanding of the things we see, we tend to generate models that seek to explain them in a simple and generalized way. In this way we can reduce the noise of our observations to general rules that “govern” them. For example, it is obvious to everyone that if we push a glass it will move in the same direction as we did. We also know that if we keep pushing it and it goes beyond the limits of the table, it will fall to the floor. But one thing is to have the intuition of what’s going to happen, and another is to have an understanding of the laws that govern that movement. In this case, they are the Newton´s Law´s of motion: $ ^{} = m*^{}$ In this way, and with only one formula, it is possible to gain an understanding that is generalizable to many aspects of reality. 3.1.0.1 Observable variables vs Latent variables Now, it is worth noting that in this case all the variables that make up our model are observable. This means that they can be measured directly. In the case of a glass, we could weigh it with a scale. Then, by pushing it, we could measure the acceleration it acquired and from these two measurements we could obtain the force we applied to it. So, every parameter of the model is fully defined. However, as we try to advance in our understanding of reality, we arrive at more and more complex models and many times we are not so lucky to be able to define them with simple observable variables. For example, this is very common in the economic sciences, where models are created with variables such as “quality of life”. Economists will try to measure this latent variable with other variables that can be observed (such as GDP per capita, schooling rate, number of hospitals for a certain number of inhabitants, etc), but that do not have an obvious and direct relationship as if they had newton’s equations. This type of latent variables are used in different models to gain greater abstraction and to be able to obtain information that is not found at first sight in the data. For example, in the case of economics, from concrete measures of a country’s economy it is possible to generalize knowledge and be able to infer an abstract variable such as quality of life. 3.1.1 Bayesian hierarchical models The Bayesian framework allows us to build statistical models that can generalize the information obtained from the data and make inferences from latent variables. A nice way to think about this kind of models is that they allow us to build our “story” about which are the variables that generate the data we are observing. Basically, they allow us to increase the “depth” of our model by indicating that the parameters of our prior distributions also follow other probability distributions. This sure is sounding very strange. Don’t worry, let’s move on to an example to clarify it. 3.1.1.1 Football analysis Let’s imagine for a moment that we are brilliant statisticians. We find ourselves looking for new interesting challenges to solve and we come across a sports bookmaker. They tell us that they want to expand into football betting and that they would like us to be able to build a model that allows them to analyze the strengths and weaknesses of English Premier League teams. They are interested because they want to be able to predict possible outcomes and thus be able to price the bets. The problem is that, as they have never worked in this sector before, they only have the results of the league matches. So what can we do? We have the data stored in a specific format called JSON, so the first thing to do is to parse and visualize it begin england_league = JSON.parsefile(&quot;matches_England.json&quot;) matches_df = DataFrame(home = [], away = [], score_home = [], score_away = []) end; begin matches = [] for match in england_league push!(matches, split(match[&quot;label&quot;], &quot;,&quot;)) end end begin for match in matches home, away = split(match[1], &quot; - &quot;) score_home, score_away = split(match[2], &quot; - &quot;) push!(matches_df,[home, away, parse(Int,score_home), parse(Int,score_away)]) end end matches_df ## 380×4 DataFrame ## Row │ home away score_home score_away ## │ Any Any Any Any ## ─────┼──────────────────────────────────────────────────────────────────────── ## 1 │ Burnley AFC Bournemouth 1 2 ## 2 │ Crystal Palace West Bromwich Albion 2 0 ## 3 │ Huddersfield Town Arsenal 0 1 ## 4 │ Liverpool Brighton &amp; Hove Albion 4 0 ## 5 │ Manchester United Watford 1 0 ## 6 │ Newcastle United Chelsea 3 0 ## 7 │ Southampton Manchester City 0 1 ## 8 │ Swansea City Stoke City 1 2 ## ⋮ │ ⋮ ⋮ ⋮ ⋮ ## 374 │ Chelsea Burnley 2 3 ## 375 │ Crystal Palace Huddersfield Town 0 3 ## 376 │ Everton Stoke City 1 0 ## 377 │ Southampton Swansea City 0 0 ## 378 │ West Bromwich Albion AFC Bournemouth 1 0 ## 379 │ Watford Liverpool 3 3 ## 380 │ Arsenal Leicester City 4 3 ## 365 rows omitted teams = unique(collect(matches_df[:,1])) ## 20-element Array{Any,1}: ## &quot;Burnley&quot; ## &quot;Crystal Palace&quot; ## &quot;Huddersfield Town&quot; ## &quot;Liverpool&quot; ## &quot;Manchester United&quot; ## &quot;Newcastle United&quot; ## &quot;Southampton&quot; ## &quot;Swansea City&quot; ## &quot;Tottenham Hotspur&quot; ## &quot;West Ham United&quot; ## &quot;Manchester City&quot; ## &quot;Leicester City&quot; ## &quot;Chelsea&quot; ## &quot;Arsenal&quot; ## &quot;Everton&quot; ## &quot;AFC Bournemouth&quot; ## &quot;Watford&quot; ## &quot;West Bromwich Albion&quot; ## &quot;Stoke City&quot; ## &quot;Brighton &amp; Hove Albion&quot; So, we have the data of the 380 matches that were played in the Premier League 2017/2018 and our challenge is to be able to analyze the characteristics of these teams. A priori it may seem that we are missing data, that with the data we have we cannot infer “characteristics” specific to each team. At most, it might be possible to see who the teams that scored the most goals, the averages of goals per game or how the positions were after the tournament, but to obtain characteristics of the teams? how could we face this problem? 3.1.1.2 Creating our stories Let’s see what information we have from our data: On one hand we have specified the names of each team and which one is local. On the other hand, we have the number of goals scored. A possible approach to this data is to realize that the goals scored by each team can be modeled with a poisson distribution. Why? You have to remember that this distribution describes “arrivals” - discrete events - in a continuum. For example, it is widely used to describe customer arrivals to a location as time passes or failures in continuous industrial processes (e.g. failure in the production of a pipe). In this particular case, we could propose that the goals scored by a team are the discrete events that occur in the time continuum that the game last: \\(Score \\sim Poisson(θ)\\) Well, we have an improvement. We’ve already told our model how to think about goals scored. Now we can use the flexibility of Bayesianism to indicate what the “goal rate” of our Poisson depends on. You can think of it literally as the number of goals a team scores per unit of time. And this is where we have to take advantage of all the information provided by the data set. As expected, this rate has to be particular to each match the team plays and take into account the opponent. We can therefore propose that the scoring rate of each team (in each particular match) depends on the “attacking power” of the team on the one hand, and the “defensive power” of the opponent on the other: \\(θ_{team1} \\sim att_{team1} + def_{team2}\\) In this way we could be capturing, from the results of each game, the attack and defence strengths of each team. Another latent variable that we could obtain, given the data, is if there is an effect that increases (or decreases) the goal rate related to whether the team is local or not. This would also help - in case there is indeed an effect - in not the attack and defence parameters be disrupted by having to “contain” that information. \\(θ_{home} \\sim home + att_{home} + def_{away}\\) \\(θ_{away} \\sim att_{away} + def_{home}\\)\" This leaves one attack and one defense parameter for each team, and a global league parameter that indicates the effect of being local on the scoring rate. 3.1.1.3 Letting the information flow We are already getting much closer to the initial goal we set. As a last step, we must be able to make the information flow between the two independent poissons that we proposed to model the score of each of the two teams that are playing. We need to do that precisely because we have proposed that the poissons are independent, but we need that when making the inference of the parameters the model can access the information from both scores so it can catch the correlation between them. In other words, we have to find a way to interconnect our model. And that is exactly what hierarchical Bayesian models allow us to do. How? By letting us choose probability distributions for the parameters that represent the characteristics of both equipment. With the addition that these parameters will share the same prior distributions. Let’s see how: The first thing to do, as we already know, is to assign the prior distributions of our attack and defense parameters. A reasonable idea would be to propose that they follow a normal distribution since it is consistent that there are some teams that have a very good defense, so the parameter would take negative values; or there may be others that have a very bad one, taking positive values (since they would “add up” to the goal rate of the opposing team). The normal distribution allows us to contemplate both cases. Now, when choosing the parameters we are not going to stop and assign fixed numbers, but we will continue to deepen the model and add another layer of distributions: \\(att_{t} \\sim Normal(μ_{att}, σ_{att})\\) \\(def_{t} \\sim Normal(μ_{def}, σ_{def})\\) Where the t sub-index is indicating us that there are a couple of these parameters for each team. Then, as a last step to have our model defined, we have to assign the priority distributions that follow the parameters of each normal distribution. We have to define our hyper priors. \\(μ_{att}, μ_{def} \\sim Normal(0, 0.1)\\) \\(σ_{att}, σ_{def} \\sim Exponential(1)\\) We must not forget the parameter that represents the advantage of being local \\(home \\sim Normal(0,1)\\) Now that our model is fully define, let’s add one last restriction to the characteristics of the teams to make it easier to compare them: subtract the average of all the attack and defence powers from each one. In this way we will have the features centred on zero, with negative values for the teams that have less attacking power than the average and positive values for those that have more. As we already said, the opposite analysis applies to the defence, negative values are the ones that will indicate that a team has a strong defence as they will be “subtracting” from the scoring rate of the opponent. This is equivalent to introducing the restriction: \\(\\sum att_{t} = 0\\) \\(\\sum def_{t} = 0\\) Let’s translate all this into Turing code: begin @model function football_matches(home_teams, away_teams, score_home, score_away, teams) #hyper priors σatt ~ Exponential(1) σdeff ~ Exponential(1) μatt ~ Normal(0,0.1) μdef ~ Normal(0,0.1) home ~ Normal(0,1) #Team-specific effects att ~ filldist(Normal(μatt, σatt), length(teams)) def ~ filldist(Normal(μatt, σdeff), length(teams)) dict = Dict{String, Int64}() for (i, team) in enumerate(teams) dict[team] = i end #Zero-sum constrains offset = mean(att) + mean(def) log_θ_home = Vector{Real}(undef, length(home_teams)) log_θ_away = Vector{Real}(undef, length(home_teams)) #Modeling score-rate and scores (as many as there were games in the league) for i in 1:length(home_teams) #score-rate log_θ_home[i] = home + att[dict[home_teams[i]]] + def[dict[away_teams[i]]] - offset log_θ_away[i] = att[dict[away_teams[i]]] + def[dict[home_teams[i]]] - offset #scores score_home[i] ~ LogPoisson(log_θ_home[i]) score_away[i] ~ LogPoisson(log_θ_away[i]) end end end ## football_matches (generic function with 1 method) As you can see, the turing code is very clear and direct. In the first block we define our hyperpriors for the distributions of the characteristics of the equipment. In the second one, we define the priors distributions that will encapsulate the information about the attack and defense powers of the teams. With the filldist function we are telling Turing that we need as many of these parameters as there are teams in the league length(teams) Then, we calculate the average of the defense and attack parameters that we are going to use to centralize those variables, and we use the LogPoisson distribution to allow the theta to take some negative value in the inference process and give more sensitivity to the parameters that make it up. As we said before, we will model the thetas for each game played in the league, that’s why the for of the last block goes from 1 to length(home_teams), which is the list that contains who was the local team of each game played. So let´s run it and see if all of this effort was worth it: model = football_matches(matches_df[:,1], matches_df[:,2], matches_df[:,3], matches_df[:,4], teams) ## DynamicPPL.Model{var&quot;#139#140&quot;,(:home_teams, :away_teams, :score_home, :score_away, :teams),(),(),NTuple{5,Array{Any,1}},Tuple{}}(:football_matches, var&quot;#139#140&quot;(), (home_teams = Any[&quot;Burnley&quot;, &quot;Crystal Palace&quot;, &quot;Huddersfield Town&quot;, &quot;Liverpool&quot;, &quot;Manchester United&quot;, &quot;Newcastle United&quot;, &quot;Southampton&quot;, &quot;Swansea City&quot;, &quot;Tottenham Hotspur&quot;, &quot;West Ham United&quot; … &quot;Manchester United&quot;, &quot;Newcastle United&quot;, &quot;Brighton &amp; Hove Albion&quot;, &quot;Chelsea&quot;, &quot;Crystal Palace&quot;, &quot;Everton&quot;, &quot;Southampton&quot;, &quot;West Bromwich Albion&quot;, &quot;Watford&quot;, &quot;Arsenal&quot;], away_teams = Any[&quot;AFC Bournemouth&quot;, &quot;West Bromwich Albion&quot;, &quot;Arsenal&quot;, &quot;Brighton &amp; Hove Albion&quot;, &quot;Watford&quot;, &quot;Chelsea&quot;, &quot;Manchester City&quot;, &quot;Stoke City&quot;, &quot;Leicester City&quot;, &quot;Everton&quot; … &quot;West Ham United&quot;, &quot;Tottenham Hotspur&quot;, &quot;Manchester City&quot;, &quot;Burnley&quot;, &quot;Huddersfield Town&quot;, &quot;Stoke City&quot;, &quot;Swansea City&quot;, &quot;AFC Bournemouth&quot;, &quot;Liverpool&quot;, &quot;Leicester City&quot;], score_home = Any[1, 2, 0, 4, 1, 3, 0, 1, 5, 3 … 4, 0, 0, 2, 0, 1, 0, 1, 3, 4], score_away = Any[2, 0, 1, 0, 0, 0, 1, 2, 4, 1 … 0, 2, 2, 3, 3, 0, 0, 0, 3, 3], teams = Any[&quot;Burnley&quot;, &quot;Crystal Palace&quot;, &quot;Huddersfield Town&quot;, &quot;Liverpool&quot;, &quot;Manchester United&quot;, &quot;Newcastle United&quot;, &quot;Southampton&quot;, &quot;Swansea City&quot;, &quot;Tottenham Hotspur&quot;, &quot;West Ham United&quot;, &quot;Manchester City&quot;, &quot;Leicester City&quot;, &quot;Chelsea&quot;, &quot;Arsenal&quot;, &quot;Everton&quot;, &quot;AFC Bournemouth&quot;, &quot;Watford&quot;, &quot;West Bromwich Albion&quot;, &quot;Stoke City&quot;, &quot;Brighton &amp; Hove Albion&quot;]), NamedTuple()) Still, the model assigns some probability to those results. Here is the information, then, you have to decide what to do with it. As data scientist that we are, our labor is to came up with possible solutions to a problem, try it, have fun, and learn from it in order to be able to came up with better solutions. It is a very good practice to constructively criticize the models that we develop. So, can you think of improvements for our model? Well, I help you with some ideas: The logic of a cup tournament is different than the league. In the first one, if you lose, you have to return to your house and the other advances to the next round. And in the league, you have to be consistent to the hole year. Maybe a draw is good for you, as your goal is to make the most difference in points. So try to extrapolate the model to other tournament maybe questionable. Other thing is that we suppose that the two matches were independent, when the second one is conditioned to the first! As the Liverpool won the first match, the Manchester had to played to the second game with the aim of making at least 3 goals while Liverpool were only focus in the defence. Anyway, we just learned that modeling is not a easy task. But the way to get better is proposing them, testing them and learning from them. In other words, the only way to learn is doing them (as any other skill in life). So, relax, model and have fun :) 3.2 Summary In this chapter we have learned about bayesian hierarchical models and how to use simulations to count different possible outcomes. First, we talked about latent variables and how important they are for gaining more abstraction in our models. After explaning the pro’s of the hierarchical models, we proceed on building a model to get a deeper understanding of the Premier League. After inferring its parameters, we have used visualizations to understand the influence of attack and defense power in the league. Finally, we ran a simulation in order to calculate the probabilities of each possible outcome of a match between Liverpool and Manchester City. 3.2.1 Bibliography Paper of Gianluca Baio and Marta A. Blangiardo Post of Daniel Weitzenfeld ### Give us feedback This book is currently in a beta version. We are looking forward to getting feedback and criticism: * Submit a GitHub issue here. * Mail us to martina.cantaro@lambdaclass.com Thank you! "]]
